{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2879,"sourceType":"datasetVersion","datasetId":1618}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T11:57:00.550236Z","iopub.execute_input":"2025-11-30T11:57:00.550521Z","iopub.status.idle":"2025-11-30T11:57:00.561505Z","shell.execute_reply.started":"2025-11-30T11:57:00.550501Z","shell.execute_reply":"2025-11-30T11:57:00.560036Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/deceptive-opinion-spam-corpus/deceptive-opinion.csv\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Load the Dataset (Deceptive Opinion Spam Corpus)\n\nIn this step, we load the Deceptive Opinion Spam dataset provided as a CSV file on Kaggle.  \nThe dataset contains reviews labeled as:\n\n- **deceptive** — fake reviews  \n- **truthful** — real reviews  \n\nWe will load the CSV, inspect its shape, check for missing values, and preview the first few rows.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/deceptive-opinion-spam-corpus/deceptive-opinion.csv\")\n\nprint(\"Dataset Shape:\", df.shape)\nprint(\"\\nColumns:\", df.columns.tolist())\n\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T11:59:28.786880Z","iopub.execute_input":"2025-11-30T11:59:28.787272Z","iopub.status.idle":"2025-11-30T11:59:28.824397Z","shell.execute_reply.started":"2025-11-30T11:59:28.787246Z","shell.execute_reply":"2025-11-30T11:59:28.823153Z"}},"outputs":[{"name":"stdout","text":"Dataset Shape: (1600, 5)\n\nColumns: ['deceptive', 'hotel', 'polarity', 'source', 'text']\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"  deceptive   hotel  polarity       source  \\\n0  truthful  conrad  positive  TripAdvisor   \n1  truthful   hyatt  positive  TripAdvisor   \n2  truthful   hyatt  positive  TripAdvisor   \n3  truthful    omni  positive  TripAdvisor   \n4  truthful   hyatt  positive  TripAdvisor   \n\n                                                text  \n0  We stayed for a one night getaway with family ...  \n1  Triple A rate with upgrade to view room was le...  \n2  This comes a little late as I'm finally catchi...  \n3  The Omni Chicago really delivers on all fronts...  \n4  I asked for a high floor away from the elevato...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>deceptive</th>\n      <th>hotel</th>\n      <th>polarity</th>\n      <th>source</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>truthful</td>\n      <td>conrad</td>\n      <td>positive</td>\n      <td>TripAdvisor</td>\n      <td>We stayed for a one night getaway with family ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>truthful</td>\n      <td>hyatt</td>\n      <td>positive</td>\n      <td>TripAdvisor</td>\n      <td>Triple A rate with upgrade to view room was le...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>truthful</td>\n      <td>hyatt</td>\n      <td>positive</td>\n      <td>TripAdvisor</td>\n      <td>This comes a little late as I'm finally catchi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>truthful</td>\n      <td>omni</td>\n      <td>positive</td>\n      <td>TripAdvisor</td>\n      <td>The Omni Chicago really delivers on all fronts...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>truthful</td>\n      <td>hyatt</td>\n      <td>positive</td>\n      <td>TripAdvisor</td>\n      <td>I asked for a high floor away from the elevato...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## Clean and Prepare the Dataset\n\nThe original dataset contains multiple columns such as hotel name, polarity, and review \nsource. For fake review detection, we only need:\n\n- `text`  → the review content  \n- `label` → deceptive (fake) or truthful (real)\n\nWe will:\n1. Extract the relevant columns  \n2. Rename the label column  \n3. Convert labels to binary (1 = deceptive, 0 = truthful)  \n4. Verify class distribution  \n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/deceptive-opinion-spam-corpus/deceptive-opinion.csv\")\n\n# Keep only necessary columns\ndf = df[['text', 'deceptive']].copy()\n\n# Rename label column\ndf.rename(columns={'deceptive': 'label'}, inplace=True)\n\n# Convert labels to binary\ndf['label'] = df['label'].map({'deceptive': 1, 'truthful': 0})\n\nprint(\"Cleaned Dataset Shape:\", df.shape)\nprint(\"\\nClass Distribution:\")\nprint(df['label'].value_counts())\n\nprint(\"\\nPreview:\")\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:00:43.884045Z","iopub.execute_input":"2025-11-30T12:00:43.884417Z","iopub.status.idle":"2025-11-30T12:00:43.939384Z","shell.execute_reply.started":"2025-11-30T12:00:43.884391Z","shell.execute_reply":"2025-11-30T12:00:43.938302Z"}},"outputs":[{"name":"stdout","text":"Cleaned Dataset Shape: (1600, 2)\n\nClass Distribution:\nlabel\n0    800\n1    800\nName: count, dtype: int64\n\nPreview:\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                text  label\n0  We stayed for a one night getaway with family ...      0\n1  Triple A rate with upgrade to view room was le...      0\n2  This comes a little late as I'm finally catchi...      0\n3  The Omni Chicago really delivers on all fronts...      0\n4  I asked for a high floor away from the elevato...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>We stayed for a one night getaway with family ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Triple A rate with upgrade to view room was le...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>This comes a little late as I'm finally catchi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The Omni Chicago really delivers on all fronts...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I asked for a high floor away from the elevato...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## Text Cleaning & Preprocessing\n\nTo prepare the reviews for machine learning, we apply standard NLP preprocessing:\n\n### Cleaning Steps:\n- Convert text to lowercase  \n- Remove punctuation  \n- Remove numbers  \n- Remove stopwords (e.g., \"the\", \"and\", \"is\")  \n- Apply lemmatization (normalize words to their root form)  \n\nThis produces cleaner text, reduces noise, and improves model accuracy.\n","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Download NLTK resources (only first time)\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\n\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text):\n    # Lowercase\n    text = text.lower()\n    \n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Remove punctuation and numbers\n    text = re.sub(r'[^a-z\\s]', ' ', text)\n    \n    # Tokenization\n    tokens = text.split()\n    \n    # Remove stopwords + lemmatization\n    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n    \n    return \" \".join(tokens)\n\n# Apply cleaning\ndf['clean_text'] = df['text'].apply(clean_text)\n\nprint(\"Sample cleaned text:\\n\")\ndf[['text', 'clean_text']].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:02:11.017331Z","iopub.execute_input":"2025-11-30T12:02:11.017700Z","iopub.status.idle":"2025-11-30T12:02:17.054003Z","shell.execute_reply.started":"2025-11-30T12:02:11.017671Z","shell.execute_reply":"2025-11-30T12:02:17.052997Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Sample cleaned text:\n\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  We stayed for a one night getaway with family ...   \n1  Triple A rate with upgrade to view room was le...   \n2  This comes a little late as I'm finally catchi...   \n3  The Omni Chicago really delivers on all fronts...   \n4  I asked for a high floor away from the elevato...   \n\n                                          clean_text  \n0  stayed one night getaway family thursday tripl...  \n1  triple rate upgrade view room less also includ...  \n2  come little late finally catching review past ...  \n3  omni chicago really delivers front spaciousnes...  \n4  asked high floor away elevator got room pleasa...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>We stayed for a one night getaway with family ...</td>\n      <td>stayed one night getaway family thursday tripl...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Triple A rate with upgrade to view room was le...</td>\n      <td>triple rate upgrade view room less also includ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>This comes a little late as I'm finally catchi...</td>\n      <td>come little late finally catching review past ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The Omni Chicago really delivers on all fronts...</td>\n      <td>omni chicago really delivers front spaciousnes...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I asked for a high floor away from the elevato...</td>\n      <td>asked high floor away elevator got room pleasa...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"## Train/Test Split & TF-IDF Vectorization\n\nNow that the dataset is cleaned, we convert the text into numerical features using \n**TF-IDF (Term Frequency – Inverse Document Frequency)**.\n\nSteps performed:\n1. Split dataset into training and testing sets (80% train, 20% test).\n2. Convert `clean_text` into TF-IDF vectors.\n3. Limit vocabulary size to avoid overfitting and speed up training.\n4. Save the vectorizer to reuse during prediction.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    df['clean_text'], \n    df['label'], \n    test_size=0.2, \n    random_state=42,\n    stratify=df['label']\n)\n\n# TF-IDF Vectorizer\ntfidf = TfidfVectorizer(\n    max_features=5000,   # limit vocabulary\n    ngram_range=(1,2),   # unigrams + bigrams improve accuracy\n    min_df=2             # ignore very rare words\n)\n\n# Fit on training data and transform both\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)\n\nprint(\"TF-IDF train shape:\", X_train_tfidf.shape)\nprint(\"TF-IDF test shape:\", X_test_tfidf.shape)\n\nX_train_tfidf[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:03:10.834187Z","iopub.execute_input":"2025-11-30T12:03:10.834684Z","iopub.status.idle":"2025-11-30T12:03:11.291279Z","shell.execute_reply.started":"2025-11-30T12:03:10.834656Z","shell.execute_reply":"2025-11-30T12:03:11.290101Z"}},"outputs":[{"name":"stdout","text":"TF-IDF train shape: (1280, 5000)\nTF-IDF test shape: (320, 5000)\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<Compressed Sparse Row sparse matrix of dtype 'float64'\n\twith 702 stored elements and shape (5, 5000)>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Model Training & Evaluation\n\nIn this step, we train multiple machine learning models:\n\n- **Logistic Regression**\n- **Support Vector Machine (SVM)**\n- **XGBoost Classifier**\n\nFor each model, we compute:\n\n- Accuracy\n- Precision\n- Recall\n- F1-score\n\nThis helps us identify the best-performing fake review detection model.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ------------------\n# Logistic Regression\n# ------------------\nlr = LogisticRegression(max_iter=2000)\nlr.fit(X_train_tfidf, y_train)\npred_lr = lr.predict(X_test_tfidf)\n\n# ------------------\n# SVM (Linear kernel)\n# ------------------\nsvm = LinearSVC()\nsvm.fit(X_train_tfidf, y_train)\npred_svm = svm.predict(X_test_tfidf)\n\n# ---------------\n# XGBoost\n# ---------------\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier(\n    eval_metric='logloss',\n    n_estimators=300,\n    learning_rate=0.1,\n    max_depth=6,\n    subsample=0.7,\n    colsample_bytree=0.7\n)\n\nxgb.fit(X_train_tfidf, y_train)\npred_xgb = xgb.predict(X_test_tfidf)\n\n# ----------------------\n# Function to evaluate\n# ----------------------\ndef evaluate_model(name, y_true, y_pred):\n    print(f\"\\n{name} Performance:\")\n    print(\"-\" * 40)\n    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n    print(\"Precision:\", precision_score(y_true, y_pred))\n    print(\"Recall   :\", recall_score(y_true, y_pred))\n    print(\"F1-score :\", f1_score(y_true, y_pred))\n\n# Print results\nevaluate_model(\"Logistic Regression\", y_test, pred_lr)\nevaluate_model(\"SVM\", y_test, pred_svm)\nevaluate_model(\"XGBoost\", y_test, pred_xgb)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:05:37.273714Z","iopub.execute_input":"2025-11-30T12:05:37.274136Z","iopub.status.idle":"2025-11-30T12:05:42.838916Z","shell.execute_reply.started":"2025-11-30T12:05:37.274110Z","shell.execute_reply":"2025-11-30T12:05:42.838257Z"}},"outputs":[{"name":"stdout","text":"\nLogistic Regression Performance:\n----------------------------------------\nAccuracy : 0.875\nPrecision: 0.8658536585365854\nRecall   : 0.8875\nF1-score : 0.8765432098765432\n\nSVM Performance:\n----------------------------------------\nAccuracy : 0.878125\nPrecision: 0.8711656441717791\nRecall   : 0.8875\nF1-score : 0.8792569659442724\n\nXGBoost Performance:\n----------------------------------------\nAccuracy : 0.825\nPrecision: 0.7988505747126436\nRecall   : 0.86875\nF1-score : 0.8323353293413174\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Save Final Model and TF-IDF Vectorizer\n\nWe select the best-performing model (SVM) and save:\n\n- The trained SVM model  \n- The fitted TF-IDF vectorizer  \n\nBoth are saved using pickle so they can be loaded inside a Flask API for real-time\nfake review detection.","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# Save SVM model\nwith open(\"final_svm_model.pkl\", \"wb\") as f:\n    pickle.dump(svm, f)\n\n# Save TF-IDF vectorizer\nwith open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n    pickle.dump(tfidf, f)\n\nprint(\"Model and vectorizer saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T12:06:51.403386Z","iopub.execute_input":"2025-11-30T12:06:51.403874Z","iopub.status.idle":"2025-11-30T12:06:51.452155Z","shell.execute_reply.started":"2025-11-30T12:06:51.403843Z","shell.execute_reply":"2025-11-30T12:06:51.450852Z"}},"outputs":[{"name":"stdout","text":"Model and vectorizer saved successfully!\n","output_type":"stream"}],"execution_count":12}]}